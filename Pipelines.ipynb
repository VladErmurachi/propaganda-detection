{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from textblob import TextBlob\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape:(16297, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US bloggers banned from entering UK\\n</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Two prominent US bloggers have been banned fro...</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pamela Geller and Robert Spencer co-founded an...</td>\n",
       "      <td>propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They were due to speak at an English Defence L...</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A government spokesman said individuals whose ...</td>\n",
       "      <td>non-propaganda</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text           label\n",
       "0              US bloggers banned from entering UK\\n  non-propaganda\n",
       "1  Two prominent US bloggers have been banned fro...  non-propaganda\n",
       "2  Pamela Geller and Robert Spencer co-founded an...      propaganda\n",
       "3  They were due to speak at an English Defence L...  non-propaganda\n",
       "4  A government spokesman said individuals whose ...  non-propaganda"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/vlad/Anaconda_notebooks/Propagada_Detection/SLC_task_df.tsv',sep = '\\t')\n",
    "df = df.drop(columns = ['Unnamed: 0', 'article'])\n",
    "print(f\"DataFrame shape:{df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data tokenization,lemmatization,lowercasing, cleaning fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "\n",
    "punctuations = string.punctuation # list of punctuation marks\n",
    "\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS # list of stop words\n",
    "\n",
    "parser = English() # english parser\n",
    "\n",
    "# fucntion for tokenization, lemmatization, lowercasing, striping white space\n",
    "def tokenizer(sentence):\n",
    "    tokens = parser(sentence)\n",
    "    tokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens ]\n",
    "    tokens = [ word for word in tokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return list of tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Custom transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "class Predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from code by @michelleful\n",
    "https://github.com/michelleful/SingaporeRoadnameOrigins\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class DataTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, vars):\n",
    "        self.vars = vars \n",
    "            \n",
    "    def transform(self, data):\n",
    "        return mydatatransform(data, self.vars)\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "class TextExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def transform(self, df):\n",
    "        return np.asarray(df[self.column_name]).astype(str)\n",
    "        \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class Apply(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, fn):\n",
    "        self.fn = np.vectorize(fn)\n",
    "        \n",
    "    def transform(self, data):\n",
    "        return self.fn(data.reshape(data.size, 1))\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Spliting the data 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "non-propaganda    11577\n",
       "propaganda        11577\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate minority and majority classes\n",
    "from sklearn.utils import resample\n",
    "nonPropaganda = df[df.label=='non-propaganda']\n",
    "propaganda = df[df.label=='propaganda']\n",
    "\n",
    "# upsample minority\n",
    "propagandaUpSampled = resample(propaganda,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(nonPropaganda), # match number in majority class\n",
    "                          random_state=0) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "df = pd.concat([propagandaUpSampled,nonPropaganda])\n",
    "\n",
    "# check new class counts\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Build in features (Character & word n-grams, tfidf transformer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-e047d3c9800b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvect_chr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'char'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvect_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "vect_chr = CountVectorizer(ngram_range=(1,4), analyzer='char')\n",
    "vect_words = CountVectorizer(tokenizer = tokenizer, ngram_range=(1,3), analyzer='word')\n",
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf_log_reg = LogisticRegression(class_weight=\"balanced\")\n",
    "clf_nb = MultinomialNB()\n",
    "clf_svc = LinearSVC(class_weight=\"balanced\")\n",
    "clf_rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Functions for features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words)) # average word length per sentence\n",
    "def count_nouns(text):\n",
    "    docs = nlp(str(text))\n",
    "    nnp_list = [word.tag_ for word in docs if word.pos_ == \"NOUN\"]\n",
    "    return len(nnp_list) # number of nouns\n",
    "def count_adjectives(text):\n",
    "    docs = nlp(str(text))\n",
    "    jj_list = [word.tag_ for word in docs if word.pos_ == \"ADJ\"]\n",
    "    return len(jj_list) # number of adjectives\n",
    "def count_verbs(text):\n",
    "    docs = nlp(str(text))\n",
    "    verb_list = [word.tag_ for word in docs if word.pos_ == \"VERB\"]\n",
    "    return len(verb_list) # number of verbs\n",
    "def number_of_entities(text):\n",
    "    docs = nlp(str(text))\n",
    "    entities = [(i, i.label_, i.label) for i in docs.ents]\n",
    "    return len(entities) # number of entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Baseline\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score: 0.5131288343558282\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda       0.71      0.51      0.60      2846\n",
      "    propaganda       0.31      0.51      0.39      1229\n",
      "\n",
      "      accuracy                           0.51      4075\n",
      "     macro avg       0.51      0.51      0.49      4075\n",
      "  weighted avg       0.59      0.51      0.53      4075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "X = df.text\n",
    "y = df.label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Create dummy classifer\n",
    "baseline = DummyClassifier(strategy='uniform', random_state=1)\n",
    "\n",
    "# \"Train\" model\n",
    "baseline.fit(X_train, y_train)\n",
    "predicted = baseline.predict(X_test)  \n",
    "baselineScore = baseline.score(X_test, y_test)  \n",
    "\n",
    "print(\"Baseline score:\", baselineScore)\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train instaces: 18523\n",
      "Number of test instances: 4631\n"
     ]
    }
   ],
   "source": [
    "train_test_set = df.loc[:]\n",
    "X = train_test_set[['text']] # Dataframe\n",
    "y = train_test_set['label'] # Series\n",
    "X_train, X_test, y_train, y_true = train_test_split(X, y, test_size=0.2,random_state = 42)\n",
    "print(f\"Number of train instaces: {len(X_train)}\")\n",
    "print(f\"Number of test instances: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Wordn n-grams vs Character n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word n-grams \n",
      "\n",
      "Logistic Regression Accuracy: 0.9054199956812783\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda       0.90      0.91      0.91      2322\n",
      "    propaganda       0.91      0.90      0.90      2309\n",
      "\n",
      "      accuracy                           0.91      4631\n",
      "     macro avg       0.91      0.91      0.91      4631\n",
      "  weighted avg       0.91      0.91      0.91      4631\n",
      "\n",
      "Character n-grams \n",
      "\n",
      "Logistic Regression Accuracy: 0.8846901317210106\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda       0.92      0.84      0.88      2322\n",
      "    propaganda       0.85      0.93      0.89      2309\n",
      "\n",
      "      accuracy                           0.88      4631\n",
      "     macro avg       0.89      0.88      0.88      4631\n",
      "  weighted avg       0.89      0.88      0.88      4631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "###### Word n-grams\n",
    "logreg = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('vect', CountVectorizer(ngram_range=(1,4), analyzer='word')),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf', LogisticRegression(class_weight = \"balanced\")),\n",
    "    ])\n",
    "model = logreg.fit(X_train, y_train)  \n",
    "predicted = model.predict(X_test)       \n",
    "print(\"Word n-grams \\n\")\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))\n",
    "###### Character n-grams\n",
    "logreg = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('vect', CountVectorizer(ngram_range=(1,6), analyzer='char')),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf', LogisticRegression(class_weight = \"balanced\")),\n",
    "    ])\n",
    "model = logreg.fit(X_train, y_train)  \n",
    "predicted = model.predict(X_test)       \n",
    "print(\"Character n-grams \\n\")\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Word n-grams + Character n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word + Character n-grams \n",
      "\n",
      "Logistic Regression Accuracy: 0.8827467069747355\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda       0.93      0.83      0.88      2322\n",
      "    propaganda       0.85      0.94      0.89      2309\n",
      "\n",
      "      accuracy                           0.88      4631\n",
      "     macro avg       0.89      0.88      0.88      4631\n",
      "  weighted avg       0.89      0.88      0.88      4631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_w_c = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('text_features', FeatureUnion([\n",
    "        ('word-n-grams', Pipeline([\n",
    "        ('cleaner', Predictors()),\n",
    "        ('vectw', CountVectorizer(tokenizer = tokenizer, ngram_range=(1,4), analyzer='word')),\n",
    "    ])),\n",
    "     ('char-n-grams', Pipeline([\n",
    "        ('vectc', CountVectorizer(ngram_range=(1,6), analyzer='char')),\n",
    "    ]))])),\n",
    "    ('tfidf',tfidf),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf' , LogisticRegression(class_weight = \"balanced\")),\n",
    "     ])\n",
    "    \n",
    "model = pipeline_w_c.fit(X_train, y_train)  \n",
    "predicted = model.predict(X_test)    \n",
    "print(\"Word + Character n-grams \\n\")\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8626646512632261\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda       0.91      0.80      0.85      2322\n",
      "    propaganda       0.82      0.92      0.87      2309\n",
      "\n",
      "      accuracy                           0.86      4631\n",
      "     macro avg       0.87      0.86      0.86      4631\n",
      "  weighted avg       0.87      0.86      0.86      4631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('text_features', FeatureUnion([\n",
    "        ('vect_c', vect_chr),\n",
    "        ('avg_word', Apply(lambda x: avg_word(x))),\n",
    "        ('nouns', Apply(lambda x: count_nouns(x))),\n",
    "        ('adjectives', Apply(lambda x: count_adjectives(x))),\n",
    "        ('verbs', Apply(lambda x: count_verbs(x))),\n",
    "        ('entities', Apply(lambda x: number_of_entities(x))),\n",
    "        ('totalCharacters', Apply(lambda s: len(s.split()))),\n",
    "        ('sentiment', Apply(lambda x: TextBlob(x).sentiment[0])),\n",
    "    ])),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf' , LogisticRegression(class_weight = \"balanced\")),   \n",
    "])\n",
    "\n",
    "model = pipeline.fit(X_train, y_train)  \n",
    "predicted = model.predict(X_test)         \n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Char. n-grams + avg.words + sentiment + NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8641762038436622\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda       0.92      0.79      0.85      2322\n",
      "    propaganda       0.82      0.93      0.87      2309\n",
      "\n",
      "      accuracy                           0.86      4631\n",
      "     macro avg       0.87      0.86      0.86      4631\n",
      "  weighted avg       0.87      0.86      0.86      4631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('text_features', FeatureUnion([\n",
    "     ('char-n-grams', Pipeline([\n",
    "        ('cleaner', Predictors()),\n",
    "        ('vectc', vect_chr),\n",
    "        ('tfidf', tfidf),\n",
    "    ])),\n",
    "     ('avgWords', Pipeline([\n",
    "        ('avg_word', Apply(lambda x: avg_word(x))),\n",
    "        ('minmax', MinMaxScaler()),\n",
    "    ])),\n",
    "    ('sent', Pipeline([\n",
    "        ('sentiment', Apply(lambda x: TextBlob(x).sentiment[0])),\n",
    "        ('minmax', MinMaxScaler()),\n",
    "    ])),\n",
    "    ('ent', Pipeline([\n",
    "        ('entities', Apply(lambda x: number_of_entities(x))),\n",
    "        ('minmax', MinMaxScaler()),\n",
    "    ])),\n",
    "       \n",
    "    ])),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf' , clf_log_reg),   \n",
    "])\n",
    "\n",
    "model = pipeline.fit(X_train, y_train)  # train the classifier\n",
    "predicted = model.predict(X_test)          # apply the model to the test data\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Char. n-grams + sentiment + NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8641762038436622\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda       0.92      0.79      0.85      2322\n",
      "    propaganda       0.82      0.93      0.87      2309\n",
      "\n",
      "      accuracy                           0.86      4631\n",
      "     macro avg       0.87      0.86      0.86      4631\n",
      "  weighted avg       0.87      0.86      0.86      4631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('text_features', FeatureUnion([\n",
    "     ('char-n-grams', Pipeline([\n",
    "        ('cleaner', Predictors()),\n",
    "        ('vectc', vect_chr),\n",
    "        ('tfidf', tfidf),\n",
    "    ])),\n",
    "    ('sent', Pipeline([\n",
    "        ('sentiment', Apply(lambda x: TextBlob(x).sentiment[0])),\n",
    "        ('minmax', MinMaxScaler()),\n",
    "    ])),\n",
    "    ('ent', Pipeline([\n",
    "        ('entities', Apply(lambda x: number_of_entities(x))),\n",
    "        ('minmax', MinMaxScaler()),\n",
    "    ])),\n",
    "    ])),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf' , clf_log_reg),   \n",
    "])\n",
    "\n",
    "model = pipeline.fit(X_train, y_train)  # train the classifier\n",
    "predicted = model.predict(X_test)          # apply the model to the test data\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Word. n-grams + sentiment + NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8808032822284604\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda       0.90      0.86      0.88      2322\n",
      "    propaganda       0.86      0.90      0.88      2309\n",
      "\n",
      "      accuracy                           0.88      4631\n",
      "     macro avg       0.88      0.88      0.88      4631\n",
      "  weighted avg       0.88      0.88      0.88      4631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('text_features', FeatureUnion([\n",
    "     ('char-n-grams', Pipeline([\n",
    "        ('cleaner', Predictors()),\n",
    "        ('vectw', CountVectorizer(tokenizer = tokenizer, ngram_range=(1,4), analyzer='word')),\n",
    "        ('tfidf', tfidf),\n",
    "    ])),\n",
    "    ('sent', Pipeline([\n",
    "        ('sentiment', Apply(lambda x: TextBlob(x).sentiment[0])),\n",
    "    ])),\n",
    "    ('ent', Pipeline([\n",
    "        ('entities', Apply(lambda x: number_of_entities(x))),\n",
    "    ])),\n",
    "    ])),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf' , clf_log_reg),   \n",
    "])\n",
    "\n",
    "model = pipeline.fit(X_train, y_train)  # train the classifier\n",
    "predicted = model.predict(X_test)          # apply the model to the test data\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Word n-grams + sentiment + NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8995897214424531\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda       0.89      0.91      0.90      2322\n",
      "    propaganda       0.91      0.89      0.90      2309\n",
      "\n",
      "      accuracy                           0.90      4631\n",
      "     macro avg       0.90      0.90      0.90      4631\n",
      "  weighted avg       0.90      0.90      0.90      4631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('text_features', FeatureUnion([\n",
    "     ('char-n-grams', Pipeline([\n",
    "        ('cleaner', Predictors()),\n",
    "        ('vectw', CountVectorizer(tokenizer = tokenizer, ngram_range=(1,4), analyzer='word')),\n",
    "    ])),\n",
    "    ('sent', Pipeline([\n",
    "        ('sentiment', Apply(lambda x: TextBlob(x).sentiment[0])),\n",
    "    ])),\n",
    "    ('ent', Pipeline([\n",
    "        ('entities', Apply(lambda x: number_of_entities(x))),\n",
    "    ])),\n",
    "    ])),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf' , clf_log_reg),   \n",
    "])\n",
    "\n",
    "model = pipeline.fit(X_train, y_train)  # train the classifier\n",
    "predicted = model.predict(X_test)          # apply the model to the test data\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Suport Vector Classifier and Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word n-grams \n",
      "\n",
      "Support Vector Classifier Accuracy: 0.8957028719499028\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda       0.91      0.88      0.89      2322\n",
      "    propaganda       0.88      0.91      0.90      2309\n",
      "\n",
      "      accuracy                           0.90      4631\n",
      "     macro avg       0.90      0.90      0.90      4631\n",
      "  weighted avg       0.90      0.90      0.90      4631\n",
      "\n",
      "Character n-grams \n",
      "\n",
      "Support Vector Classifier Accuracy: 0.8810192183113799\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda       0.92      0.83      0.88      2322\n",
      "    propaganda       0.85      0.93      0.89      2309\n",
      "\n",
      "      accuracy                           0.88      4631\n",
      "     macro avg       0.88      0.88      0.88      4631\n",
      "  weighted avg       0.88      0.88      0.88      4631\n",
      "\n",
      "Word n-grams \n",
      "\n",
      "Naive Bayes Classifier Accuracy: 0.8129993521917512\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda       0.92      0.69      0.79      2322\n",
      "    propaganda       0.75      0.94      0.83      2309\n",
      "\n",
      "      accuracy                           0.81      4631\n",
      "     macro avg       0.83      0.81      0.81      4631\n",
      "  weighted avg       0.83      0.81      0.81      4631\n",
      "\n",
      "Character n-grams \n",
      "\n",
      "Naive Bayes Classifier Accuracy: 0.8101921831137984\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda       0.89      0.71      0.79      2322\n",
      "    propaganda       0.76      0.91      0.83      2309\n",
      "\n",
      "      accuracy                           0.81      4631\n",
      "     macro avg       0.82      0.81      0.81      4631\n",
      "  weighted avg       0.82      0.81      0.81      4631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Word n-grams\n",
    "logreg = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('vect', CountVectorizer(ngram_range=(1,4), analyzer='word')),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf', clf_svc),\n",
    "    ])\n",
    "model = logreg.fit(X_train, y_train)  \n",
    "predicted = model.predict(X_test)       \n",
    "print(\"Word n-grams \\n\")\n",
    "print(\"Support Vector Classifier Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))\n",
    "###### Character n-grams\n",
    "logreg = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('vect', CountVectorizer(ngram_range=(1,6), analyzer='char')),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf', clf_svc),\n",
    "    ])\n",
    "model = logreg.fit(X_train, y_train)  \n",
    "predicted = model.predict(X_test)       \n",
    "print(\"Character n-grams \\n\")\n",
    "print(\"Support Vector Classifier Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))\n",
    "###### Word n-grams\n",
    "logreg = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('vect', CountVectorizer(ngram_range=(1,4), analyzer='word')),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf', clf_nb),\n",
    "    ])\n",
    "model = logreg.fit(X_train, y_train)  \n",
    "predicted = model.predict(X_test)       \n",
    "print(\"Word n-grams \\n\")\n",
    "print(\"Naive Bayes Classifier Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))\n",
    "###### Character n-grams\n",
    "logreg = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('vect', CountVectorizer(ngram_range=(1,6), analyzer='char')),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf', clf_nb),\n",
    "    ])\n",
    "model = logreg.fit(X_train, y_train)  \n",
    "predicted = model.predict(X_test)       \n",
    "print(\"Character n-grams \\n\")\n",
    "print(\"Naive Bayes Classifier Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Random Forrest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word n-grams \n",
      "\n",
      "Random Forrest Classifier Accuracy: 0.9252861153098683\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda       0.90      0.96      0.93      2322\n",
      "    propaganda       0.95      0.90      0.92      2309\n",
      "\n",
      "      accuracy                           0.93      4631\n",
      "     macro avg       0.93      0.93      0.93      4631\n",
      "  weighted avg       0.93      0.93      0.93      4631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Word n-grams\n",
    "rf = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('vect', CountVectorizer(ngram_range=(1,4), analyzer='word')),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf', clf_rf),\n",
    "    ])\n",
    "model = rf.fit(X_train, y_train)  \n",
    "predicted = model.predict(X_test)       \n",
    "print(\"Word n-grams \\n\")\n",
    "print(\"Random Forrest Classifier Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))\n",
    "###### Character n-grams\n",
    "rf = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('vect', CountVectorizer(ngram_range=(1,6), analyzer='char')),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf', clf_rf),\n",
    "    ])\n",
    "model = rf.fit(X_train, y_train)  \n",
    "predicted = model.predict(X_test)       \n",
    "print(\"Character n-grams \\n\")\n",
    "print(\"Random Forrest Classifier Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import joblib\n",
    "joblib.dump(model, \"logReg_binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>  Task II Fragment Level Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>text</th>\n",
       "      <th>lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>article702077434.txt</td>\n",
       "      <td>I ruined my life</td>\n",
       "      <td>Thought-terminating_Cliches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>article703056647.txt</td>\n",
       "      <td>local continue to balk that their rituals have...</td>\n",
       "      <td>Doubt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4943</th>\n",
       "      <td>article774007496.txt</td>\n",
       "      <td>beaten her to death</td>\n",
       "      <td>Loaded_Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>article707451080.txt</td>\n",
       "      <td>Weinstein-level” sexual assault</td>\n",
       "      <td>Name_Calling,Labeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366</th>\n",
       "      <td>article765982381.txt</td>\n",
       "      <td>Where is the voice of conscience to condemn wh...</td>\n",
       "      <td>Doubt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   article                                               text  \\\n",
       "924   article702077434.txt                                   I ruined my life   \n",
       "1620  article703056647.txt  local continue to balk that their rituals have...   \n",
       "4943  article774007496.txt                                beaten her to death   \n",
       "1850  article707451080.txt                   Weinstein-level” sexual assault    \n",
       "4366  article765982381.txt  Where is the voice of conscience to condemn wh...   \n",
       "\n",
       "                            lable  \n",
       "924   Thought-terminating_Cliches  \n",
       "1620                        Doubt  \n",
       "4943              Loaded_Language  \n",
       "1850        Name_Calling,Labeling  \n",
       "4366                        Doubt  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "Corpus = pd.read_csv(\"FLC_task2_df.tsv\",sep = '\\t')\n",
    "Corpus = Corpus.drop(columns = ['Unnamed: 0'])\n",
    "Corpus = shuffle(Corpus)\n",
    "Corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loaded_Language                       1554\n",
       "Name_Calling,Labeling                  678\n",
       "Repetition                             549\n",
       "Flag-Waving                            412\n",
       "Causal_Oversimplification              342\n",
       "Exaggeration,Minimisation              342\n",
       "Doubt                                  311\n",
       "Appeal_to_fear-prejudice               276\n",
       "Slogans                                234\n",
       "Appeal_to_Authority                    224\n",
       "Black-and-White_Fallacy                203\n",
       "Whataboutism,Straw_Men,Red_Herring     188\n",
       "Thought-terminating_Cliches            133\n",
       "Bandwagon,Reductio_ad_hitlerum         120\n",
       "Name: lable, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Corpus['lable'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score: 0.07614942528735633\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "               Appeal_to_Authority       0.06      0.14      0.09        49\n",
      "          Appeal_to_fear-prejudice       0.04      0.05      0.05        76\n",
      "    Bandwagon,Reductio_ad_hitlerum       0.01      0.04      0.01        28\n",
      "           Black-and-White_Fallacy       0.01      0.02      0.01        42\n",
      "         Causal_Oversimplification       0.08      0.09      0.09        86\n",
      "                             Doubt       0.06      0.08      0.07        73\n",
      "         Exaggeration,Minimisation       0.09      0.10      0.10        97\n",
      "                       Flag-Waving       0.08      0.08      0.08        96\n",
      "                   Loaded_Language       0.33      0.07      0.12       398\n",
      "             Name_Calling,Labeling       0.15      0.09      0.11       161\n",
      "                        Repetition       0.06      0.04      0.05       151\n",
      "                           Slogans       0.05      0.09      0.07        53\n",
      "       Thought-terminating_Cliches       0.04      0.13      0.06        31\n",
      "Whataboutism,Straw_Men,Red_Herring       0.04      0.08      0.05        51\n",
      "\n",
      "                          accuracy                           0.08      1392\n",
      "                         macro avg       0.08      0.08      0.07      1392\n",
      "                      weighted avg       0.15      0.08      0.08      1392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "X = Corpus.text\n",
    "y = Corpus.lable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Create dummy classifer\n",
    "baseline = DummyClassifier(strategy='uniform', random_state=1)\n",
    "\n",
    "# \"Train\" model\n",
    "baseline.fit(X_train, y_train)\n",
    "predicted = baseline.predict(X_test)  \n",
    "baselineScore = baseline.score(X_test, y_test)  \n",
    "\n",
    "print(\"Baseline score:\", baselineScore)\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train instaces: 4452\n",
      "Number of test instances: 1114\n"
     ]
    }
   ],
   "source": [
    "train_test_set = Corpus.loc[:]\n",
    "X = train_test_set[['text']] # Dataframe\n",
    "y = train_test_set['lable'] # Series\n",
    "X_train, X_test, y_train, y_true = train_test_split(X, y, test_size=0.2,random_state = 42)\n",
    "print(f\"Number of train instaces: {len(X_train)}\")\n",
    "print(f\"Number of test instances: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Word vs Character n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word n-grams\n",
      "Logistic Regression Accuracy: 0.6032315978456014\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "               Appeal_to_Authority       0.65      0.60      0.62        58\n",
      "          Appeal_to_fear-prejudice       0.35      0.25      0.29        52\n",
      "    Bandwagon,Reductio_ad_hitlerum       0.77      0.62      0.69        32\n",
      "           Black-and-White_Fallacy       0.82      0.86      0.84        36\n",
      "         Causal_Oversimplification       0.66      0.80      0.73        76\n",
      "                             Doubt       0.47      0.58      0.52        55\n",
      "         Exaggeration,Minimisation       0.31      0.26      0.28        72\n",
      "                       Flag-Waving       0.70      0.76      0.73        78\n",
      "                   Loaded_Language       0.63      0.67      0.65       296\n",
      "             Name_Calling,Labeling       0.56      0.45      0.50       137\n",
      "                        Repetition       0.53      0.51      0.52       113\n",
      "                           Slogans       0.73      0.80      0.77        41\n",
      "       Thought-terminating_Cliches       0.53      0.78      0.63        27\n",
      "Whataboutism,Straw_Men,Red_Herring       0.94      0.73      0.82        41\n",
      "\n",
      "                          accuracy                           0.60      1114\n",
      "                         macro avg       0.62      0.62      0.61      1114\n",
      "                      weighted avg       0.60      0.60      0.60      1114\n",
      "\n",
      "Character n-grams\n",
      "Logistic Regression Accuracy: 0.6355475763016158\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "               Appeal_to_Authority       0.70      0.60      0.65        58\n",
      "          Appeal_to_fear-prejudice       0.37      0.38      0.38        52\n",
      "    Bandwagon,Reductio_ad_hitlerum       0.80      0.62      0.70        32\n",
      "           Black-and-White_Fallacy       0.88      0.83      0.86        36\n",
      "         Causal_Oversimplification       0.73      0.92      0.81        76\n",
      "                             Doubt       0.50      0.56      0.53        55\n",
      "         Exaggeration,Minimisation       0.42      0.35      0.38        72\n",
      "                       Flag-Waving       0.68      0.79      0.73        78\n",
      "                   Loaded_Language       0.65      0.67      0.66       296\n",
      "             Name_Calling,Labeling       0.58      0.53      0.55       137\n",
      "                        Repetition       0.54      0.54      0.54       113\n",
      "                           Slogans       0.84      0.78      0.81        41\n",
      "       Thought-terminating_Cliches       0.70      0.78      0.74        27\n",
      "Whataboutism,Straw_Men,Red_Herring       0.97      0.76      0.85        41\n",
      "\n",
      "                          accuracy                           0.64      1114\n",
      "                         macro avg       0.67      0.65      0.66      1114\n",
      "                      weighted avg       0.64      0.64      0.63      1114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Word n-grams \\n\")\n",
    "pipeline = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('cleaner', Predictors()),\n",
    "    ('vectc', CountVectorizer(ngram_range=(1,4), analyzer='word')),\n",
    "    ('tfidf', tfidf),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf', LogisticRegression(class_weight = \"balanced\")),   \n",
    "])\n",
    "\n",
    "model = pipeline.fit(X_train, y_train) \n",
    "predicted = model.predict(X_test)         \n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "print(\"Character n-grams \\n\")\n",
    "pipeline = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    #('cleaner', Predictors()),\n",
    "    ('vectc', CountVectorizer(ngram_range=(1,6), analyzer='char')),\n",
    "    ('tfidf', tfidf),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf', LogisticRegression(class_weight = \"balanced\")),   \n",
    "])\n",
    "\n",
    "model = pipeline.fit(X_train, y_train) \n",
    "predicted = model.predict(X_test)         \n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Word + Character n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word + Character n-grams \n",
      "\n",
      "Logistic Regression Accuracy: 0.6391382405745063\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "               Appeal_to_Authority       0.69      0.59      0.64        58\n",
      "          Appeal_to_fear-prejudice       0.39      0.40      0.40        52\n",
      "    Bandwagon,Reductio_ad_hitlerum       0.80      0.62      0.70        32\n",
      "           Black-and-White_Fallacy       0.88      0.83      0.86        36\n",
      "         Causal_Oversimplification       0.72      0.91      0.80        76\n",
      "                             Doubt       0.50      0.56      0.53        55\n",
      "         Exaggeration,Minimisation       0.42      0.35      0.38        72\n",
      "                       Flag-Waving       0.67      0.79      0.73        78\n",
      "                   Loaded_Language       0.66      0.68      0.67       296\n",
      "             Name_Calling,Labeling       0.59      0.53      0.56       137\n",
      "                        Repetition       0.53      0.56      0.55       113\n",
      "                           Slogans       0.84      0.76      0.79        41\n",
      "       Thought-terminating_Cliches       0.72      0.78      0.75        27\n",
      "Whataboutism,Straw_Men,Red_Herring       1.00      0.73      0.85        41\n",
      "\n",
      "                          accuracy                           0.64      1114\n",
      "                         macro avg       0.67      0.65      0.66      1114\n",
      "                      weighted avg       0.64      0.64      0.64      1114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_w_c = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('text_features', FeatureUnion([\n",
    "        ('word-n-grams', Pipeline([\n",
    "        ('cleaner', Predictors()),\n",
    "        ('vectw', CountVectorizer(tokenizer = tokenizer, ngram_range=(1,4), analyzer='word')),\n",
    "    ])),\n",
    "     ('char-n-grams', Pipeline([\n",
    "        ('cleaner', Predictors()),\n",
    "        ('vectc', CountVectorizer(ngram_range=(1,6), analyzer='char')),\n",
    "    ]))])),\n",
    "    ('tfidf',tfidf),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf' , LogisticRegression(class_weight = \"balanced\")),\n",
    "     ])\n",
    "    \n",
    "model = pipeline_w_c.fit(X_train, y_train)  \n",
    "predicted = model.predict(X_test)    \n",
    "print(\"Word + Character n-grams \\n\")\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Chararacter n-grams + sentiment + NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6409335727109515\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "               Appeal_to_Authority       0.65      0.62      0.64        58\n",
      "          Appeal_to_fear-prejudice       0.39      0.40      0.40        52\n",
      "    Bandwagon,Reductio_ad_hitlerum       0.80      0.62      0.70        32\n",
      "           Black-and-White_Fallacy       0.91      0.83      0.87        36\n",
      "         Causal_Oversimplification       0.73      0.91      0.81        76\n",
      "                             Doubt       0.53      0.56      0.54        55\n",
      "         Exaggeration,Minimisation       0.41      0.35      0.38        72\n",
      "                       Flag-Waving       0.68      0.78      0.73        78\n",
      "                   Loaded_Language       0.67      0.67      0.67       296\n",
      "             Name_Calling,Labeling       0.59      0.55      0.57       137\n",
      "                        Repetition       0.53      0.56      0.54       113\n",
      "                           Slogans       0.84      0.76      0.79        41\n",
      "       Thought-terminating_Cliches       0.72      0.78      0.75        27\n",
      "Whataboutism,Straw_Men,Red_Herring       0.97      0.76      0.85        41\n",
      "\n",
      "                          accuracy                           0.64      1114\n",
      "                         macro avg       0.67      0.65      0.66      1114\n",
      "                      weighted avg       0.64      0.64      0.64      1114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('text_features', FeatureUnion([\n",
    "     ('char-n-grams', Pipeline([\n",
    "        ('cleaner', Predictors()),\n",
    "        ('vectc', CountVectorizer(tokenizer = tokenizer, ngram_range=(1,6), analyzer='char')),\n",
    "        ('tfidf', tfidf),\n",
    "    ])),\n",
    "    ('sent', Pipeline([\n",
    "        ('sentiment', Apply(lambda x: TextBlob(x).sentiment[0])),\n",
    "    ])),\n",
    "    ('ent', Pipeline([\n",
    "        ('entities', Apply(lambda x: number_of_entities(x))),\n",
    "\n",
    "    ])),\n",
    "       \n",
    "    ])),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf' , clf_log_reg),   \n",
    "])\n",
    "\n",
    "model = pipeline.fit(X_train, y_train)  # train the classifier\n",
    "predicted = model.predict(X_test)          # apply the model to the test data\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6166965888689407\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "               Appeal_to_Authority       0.61      0.60      0.61        58\n",
      "          Appeal_to_fear-prejudice       0.30      0.25      0.27        52\n",
      "    Bandwagon,Reductio_ad_hitlerum       0.83      0.62      0.71        32\n",
      "           Black-and-White_Fallacy       0.77      0.83      0.80        36\n",
      "         Causal_Oversimplification       0.83      0.84      0.84        76\n",
      "                             Doubt       0.52      0.42      0.46        55\n",
      "         Exaggeration,Minimisation       0.37      0.39      0.38        72\n",
      "                       Flag-Waving       0.72      0.81      0.76        78\n",
      "                   Loaded_Language       0.64      0.65      0.64       296\n",
      "             Name_Calling,Labeling       0.55      0.51      0.53       137\n",
      "                        Repetition       0.50      0.58      0.54       113\n",
      "                           Slogans       0.76      0.78      0.77        41\n",
      "       Thought-terminating_Cliches       0.70      0.78      0.74        27\n",
      "Whataboutism,Straw_Men,Red_Herring       0.86      0.76      0.81        41\n",
      "\n",
      "                          accuracy                           0.62      1114\n",
      "                         macro avg       0.64      0.63      0.63      1114\n",
      "                      weighted avg       0.62      0.62      0.61      1114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('text_features', FeatureUnion([\n",
    "        ('vect_c', vect_chr),\n",
    "        ('avg_word', Apply(lambda x: avg_word(x))),\n",
    "        ('nouns', Apply(lambda x: count_nouns(x))),\n",
    "        ('adjectives', Apply(lambda x: count_adjectives(x))),\n",
    "        ('verbs', Apply(lambda x: count_verbs(x))),\n",
    "        ('entities', Apply(lambda x: number_of_entities(x))),\n",
    "        ('totalCharacters', Apply(lambda s: len(s.split()))),\n",
    "        ('sentiment', Apply(lambda x: TextBlob(x).sentiment[0])),\n",
    "    ])),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf' , LogisticRegression(class_weight = \"balanced\")),   \n",
    "])\n",
    "\n",
    "model = pipeline.fit(X_train, y_train)  \n",
    "predicted = model.predict(X_test)         \n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Suport Vector Classifier and Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word n-grams \n",
      "\n",
      "Support Vector Classifier Accuracy: 0.5763016157989228\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "               Appeal_to_Authority       0.70      0.55      0.62        58\n",
      "          Appeal_to_fear-prejudice       0.23      0.12      0.15        52\n",
      "    Bandwagon,Reductio_ad_hitlerum       0.77      0.62      0.69        32\n",
      "           Black-and-White_Fallacy       0.81      0.81      0.81        36\n",
      "         Causal_Oversimplification       0.92      0.64      0.76        76\n",
      "                             Doubt       0.57      0.24      0.33        55\n",
      "         Exaggeration,Minimisation       0.33      0.26      0.29        72\n",
      "                       Flag-Waving       0.68      0.81      0.74        78\n",
      "                   Loaded_Language       0.54      0.76      0.63       296\n",
      "             Name_Calling,Labeling       0.56      0.36      0.44       137\n",
      "                        Repetition       0.43      0.50      0.47       113\n",
      "                           Slogans       0.65      0.73      0.69        41\n",
      "       Thought-terminating_Cliches       0.53      0.70      0.60        27\n",
      "Whataboutism,Straw_Men,Red_Herring       0.94      0.76      0.84        41\n",
      "\n",
      "                          accuracy                           0.58      1114\n",
      "                         macro avg       0.62      0.56      0.58      1114\n",
      "                      weighted avg       0.58      0.58      0.56      1114\n",
      "\n",
      "Character n-grams \n",
      "\n",
      "Support Vector Classifier Accuracy: 0.5933572710951526\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "               Appeal_to_Authority       0.67      0.57      0.62        58\n",
      "          Appeal_to_fear-prejudice       0.27      0.19      0.22        52\n",
      "    Bandwagon,Reductio_ad_hitlerum       0.81      0.66      0.72        32\n",
      "           Black-and-White_Fallacy       0.74      0.81      0.77        36\n",
      "         Causal_Oversimplification       0.81      0.75      0.78        76\n",
      "                             Doubt       0.51      0.40      0.45        55\n",
      "         Exaggeration,Minimisation       0.37      0.31      0.33        72\n",
      "                       Flag-Waving       0.64      0.78      0.71        78\n",
      "                   Loaded_Language       0.59      0.68      0.63       296\n",
      "             Name_Calling,Labeling       0.55      0.44      0.49       137\n",
      "                        Repetition       0.48      0.55      0.51       113\n",
      "                           Slogans       0.67      0.76      0.71        41\n",
      "       Thought-terminating_Cliches       0.66      0.78      0.71        27\n",
      "Whataboutism,Straw_Men,Red_Herring       0.84      0.76      0.79        41\n",
      "\n",
      "                          accuracy                           0.59      1114\n",
      "                         macro avg       0.62      0.60      0.60      1114\n",
      "                      weighted avg       0.59      0.59      0.59      1114\n",
      "\n",
      "Word n-grams \n",
      "\n",
      "Naive Bayes Classifier Accuracy: 0.5861759425493717\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "               Appeal_to_Authority       0.65      0.57      0.61        58\n",
      "          Appeal_to_fear-prejudice       0.31      0.23      0.26        52\n",
      "    Bandwagon,Reductio_ad_hitlerum       0.80      0.62      0.70        32\n",
      "           Black-and-White_Fallacy       0.76      0.78      0.77        36\n",
      "         Causal_Oversimplification       0.55      0.87      0.68        76\n",
      "                             Doubt       0.45      0.45      0.45        55\n",
      "         Exaggeration,Minimisation       0.38      0.21      0.27        72\n",
      "                       Flag-Waving       0.62      0.77      0.69        78\n",
      "                   Loaded_Language       0.54      0.83      0.66       296\n",
      "             Name_Calling,Labeling       0.66      0.29      0.40       137\n",
      "                        Repetition       0.63      0.34      0.44       113\n",
      "                           Slogans       1.00      0.66      0.79        41\n",
      "       Thought-terminating_Cliches       0.78      0.67      0.72        27\n",
      "Whataboutism,Straw_Men,Red_Herring       0.87      0.63      0.73        41\n",
      "\n",
      "                          accuracy                           0.59      1114\n",
      "                         macro avg       0.64      0.57      0.58      1114\n",
      "                      weighted avg       0.60      0.59      0.57      1114\n",
      "\n",
      "Character n-grams \n",
      "\n",
      "Naive Bayes Classifier Accuracy: 0.5368043087971275\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "               Appeal_to_Authority       0.50      0.57      0.53        58\n",
      "          Appeal_to_fear-prejudice       0.30      0.35      0.32        52\n",
      "    Bandwagon,Reductio_ad_hitlerum       0.64      0.66      0.65        32\n",
      "           Black-and-White_Fallacy       0.64      0.69      0.67        36\n",
      "         Causal_Oversimplification       0.39      0.88      0.54        76\n",
      "                             Doubt       0.30      0.49      0.38        55\n",
      "         Exaggeration,Minimisation       0.33      0.21      0.25        72\n",
      "                       Flag-Waving       0.58      0.78      0.67        78\n",
      "                   Loaded_Language       0.61      0.58      0.59       296\n",
      "             Name_Calling,Labeling       0.68      0.39      0.50       137\n",
      "                        Repetition       0.69      0.40      0.51       113\n",
      "                           Slogans       0.91      0.51      0.66        41\n",
      "       Thought-terminating_Cliches       0.72      0.48      0.58        27\n",
      "Whataboutism,Straw_Men,Red_Herring       0.79      0.66      0.72        41\n",
      "\n",
      "                          accuracy                           0.54      1114\n",
      "                         macro avg       0.58      0.55      0.54      1114\n",
      "                      weighted avg       0.58      0.54      0.54      1114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Word n-grams\n",
    "logreg = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('vect', CountVectorizer(ngram_range=(1,4), analyzer='word')),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf', clf_svc),\n",
    "    ])\n",
    "model = logreg.fit(X_train, y_train)  \n",
    "predicted = model.predict(X_test)       \n",
    "print(\"Word n-grams \\n\")\n",
    "print(\"Support Vector Classifier Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))\n",
    "###### Character n-grams\n",
    "logreg = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('vect', CountVectorizer(ngram_range=(1,6), analyzer='char')),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf', clf_svc),\n",
    "    ])\n",
    "model = logreg.fit(X_train, y_train)  \n",
    "predicted = model.predict(X_test)       \n",
    "print(\"Character n-grams \\n\")\n",
    "print(\"Support Vector Classifier Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))\n",
    "###### Word n-grams\n",
    "logreg = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('vect', CountVectorizer(ngram_range=(1,4), analyzer='word')),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf', clf_nb),\n",
    "    ])\n",
    "model = logreg.fit(X_train, y_train)  \n",
    "predicted = model.predict(X_test)       \n",
    "print(\"Word n-grams \\n\")\n",
    "print(\"Naive Bayes Classifier Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))\n",
    "###### Character n-grams\n",
    "logreg = Pipeline([\n",
    "    ('text_extractor', TextExtractor('text')), \n",
    "    ('vect', CountVectorizer(ngram_range=(1,6), analyzer='char')),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf', clf_nb),\n",
    "    ])\n",
    "model = logreg.fit(X_train, y_train)  \n",
    "predicted = model.predict(X_test)       \n",
    "print(\"Character n-grams \\n\")\n",
    "print(\"Naive Bayes Classifier Accuracy:\",metrics.accuracy_score(y_true, predicted))\n",
    "print(classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Fragment level classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17143, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/vlad/Anaconda_notebooks/Propagada_Detection/Fragment_classification.tsv',sep = '\\t')\n",
    "df = df.drop(columns = ['Unnamed: 0'])\n",
    "df = shuffle(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.text\n",
    "y = df.label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word n-grams \n",
      "\n",
      "Logistic Regression Accuracy: 0.7312883435582822\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda       0.79      0.83      0.81      2846\n",
      "    propaganda       0.56      0.50      0.53      1229\n",
      "\n",
      "      accuracy                           0.73      4075\n",
      "     macro avg       0.68      0.67      0.67      4075\n",
      "  weighted avg       0.72      0.73      0.73      4075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "###### Word n-grams\n",
    "logreg = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,3), analyzer='word')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf', LogisticRegression(class_weight = \"balanced\")),\n",
    "    ])\n",
    "model = logreg.fit(X_train, y_train)  \n",
    "predicted = model.predict(X_test)       \n",
    "print(\"Word n-grams \\n\")\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logReg_fragment_word']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model, \"logReg_fragment_word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word n-grams \n",
      "\n",
      "Logistic Regression Accuracy: 0.7312883435582822\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "non-propaganda       0.79      0.83      0.81      2846\n",
      "    propaganda       0.56      0.49      0.53      1229\n",
      "\n",
      "      accuracy                           0.73      4075\n",
      "     macro avg       0.68      0.66      0.67      4075\n",
      "  weighted avg       0.72      0.73      0.73      4075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "###### Word n-grams\n",
    "logreg = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,3), analyzer='word')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('minmax', MaxAbsScaler()),\n",
    "    ('clf', clf_svc),\n",
    "    ])\n",
    "model = logreg.fit(X_train, y_train)  \n",
    "predicted = model.predict(X_test)       \n",
    "print(\"Word n-grams \\n\")\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logReg_fragment2']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
